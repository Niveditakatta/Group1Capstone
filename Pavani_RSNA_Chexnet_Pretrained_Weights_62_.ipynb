{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RSNA_Chexnet_Pretrained_Weights_62%.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAdu19R71YlQ",
        "colab_type": "code",
        "outputId": "6e41756a-ee29-4cea-8177-5eabac708bbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "pip install pydicom"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydicom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/d5/da1fdf3b967e324ee47a7ad9553c9b94c1193b6b98afd9eeda0efb76b9f7/pydicom-1.3.0-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 42.5MB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-1.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ7mvOAk1ddM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pydicom\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import os\n",
        "from matplotlib.patches import Rectangle\n",
        "import seaborn as sns\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI8pfW4E1gIo",
        "colab_type": "code",
        "outputId": "b756a386-53e4-4641-c753-e7933340f98f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFUDqVxj1qKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import keras\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "#from generator import DataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QWEgAfk1yke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D, Dropout, Dense,Conv2D\n",
        "from keras.layers import BatchNormalization,Activation\n",
        "from keras.layers import MaxPooling2D,GlobalAveragePooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.optimizers import adam\n",
        "from keras import optimizers\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqbWhYpj14jF",
        "colab_type": "code",
        "outputId": "37787606-4706-4dfe-82e2-a39fa4f012de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd drive/My Drive/Capstone"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Capstone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNBsge5E17g6",
        "colab_type": "code",
        "outputId": "3c266ed7-1dcf-411d-b18d-5a3fca765912",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " backup\n",
            " cfg\n",
            " chexnet_weights_pnemonia_1.h5\n",
            " chexnet_weights_pnemonia_2.h5\n",
            " chexnet_weights_pnemonia_adadelta.h5\n",
            " chexnet_weights_pnemonia_adagrad.h5\n",
            " chexnet_weights_pnemonia_adam1.h5\n",
            " chexnet_weights_pnemonia_adam2.h5\n",
            " chexnet_weights_pnemonia_adam.h5\n",
            " chexnet_weights_pnemonia.h5\n",
            " chexnet_weights_pnemonia_rmsprop.h5\n",
            " chexnet_weights_pnemonia_sgd.h5\n",
            " Classfiication_Report_lavasesha.ipynb\n",
            "'Classification Model.ipynb'\n",
            " Classification_Model.ipynb\n",
            " darknet\n",
            " darknet_gpu\n",
            " FinalSubmission\n",
            " fulltrainlabel_adam.h5\n",
            " images\n",
            " InceptionV3_rmsprop.h5\n",
            " keras-retinanet-master\n",
            " labels\n",
            " metadata\n",
            " model_pnemonia_adadelta.h5\n",
            " model_pnemonia_adagrad.h5\n",
            " model_pnemonia_adam1.h5\n",
            " model_pnemonia_adam2.h5\n",
            " model_pnemonia_adam.h5\n",
            " model_pnemonia_adamlr.h5\n",
            " model_pnemonia.h5\n",
            " model_pnemonia_rmsprop.h5\n",
            " model_pnemonia_sgd.h5\n",
            " Models\n",
            " PretrainedWeights\n",
            " RSNA_Classification_EDA_Imran_CheXnet_75ValAccuracy.ipynb\n",
            " RSNA_EDA_lavasesha.ipynb\n",
            " RSNA_EDA_Malini.ipynb\n",
            " rsna_pneumonia_detection_challenge_kaggle.ipynb\n",
            " RSNA_Pretrained_Weights_Imran_13072019.ipynb\n",
            " RSNA_Pretrained_Weights_Imran1.ipynb\n",
            " RSNA_Pretrained_Weights_Imran.ipynb\n",
            " RSNAYOLO.ipynb\n",
            " stage_2_detailed_class_info.csv\n",
            " stage_2_detailed_class_info.csv.zip\n",
            " stage_2_sample_submission.csv\n",
            " stage2_test_images\n",
            " stage2_train_images\n",
            " stage_2_train_labels.csv\n",
            " stage_2_train_labels.csv.zip\n",
            " test_png\n",
            " train_images_png\n",
            " train_png\n",
            " transfer_all_sgd.h5\n",
            " validation_png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl8n6Dzl2F2s",
        "colab_type": "code",
        "outputId": "1098741d-2000-41d2-b8f7-fc4e4b55758f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        #horizontal_flip=True\n",
        "         )\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator_1 = train_datagen.flow_from_directory(\n",
        "        '/content/drive/My Drive/Capstone/train_png/',\n",
        "        target_size=(256, 256),\n",
        "        batch_size=32,color_mode='rgb',\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator_1 = test_datagen.flow_from_directory(\n",
        "        '/content/drive/My Drive/Capstone/validation_png/',\n",
        "        target_size=(256, 256),\n",
        "        batch_size=32,color_mode='rgb',\n",
        "        class_mode='binary')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 4838 images belonging to 2 classes.\n",
            "Found 441 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rerVkVxj2UNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input_shape=(256,256,3)\n",
        "# img_in = Input(input_shape)              #input of model \n",
        "# model = DenseNet121(include_top= False , # remove  the 3 fully-connected layers at the top of the network\n",
        "#                 weights='/content/drive/My Drive/Colab Notebooks/RSNA/rsna-pneumonia-detection-challenge/brucechou1983_CheXNet_Keras_0.3.0_weights.h5',      # pre train weight \n",
        "#                 input_tensor= img_in, \n",
        "#                 input_shape= input_shape,\n",
        "#                 pooling ='max') \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUvdyvNJ3SpE",
        "colab_type": "code",
        "outputId": "d87f83ed-7ac4-47a9-86c2-145c76963f55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "input_shape=(256,256,3)\n",
        "img_in = Input(input_shape)              #input of model \n",
        "model = DenseNet121(include_top= False , # remove  the 3 fully-connected layers at the top of the network\n",
        "                weights= None,      # pre train weight \n",
        "                input_tensor= img_in, \n",
        "                input_shape= input_shape,\n",
        "                pooling ='max') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0713 04:05:55.337440 140211364968320 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0713 04:05:55.376651 140211364968320 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0713 04:05:55.387323 140211364968320 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0713 04:05:55.419352 140211364968320 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0713 04:05:55.420265 140211364968320 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0713 04:05:58.204353 140211364968320 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0713 04:05:58.284463 140211364968320 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0713 04:05:59.341844 140211364968320 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv_qp1h1Yu9R",
        "colab_type": "code",
        "outputId": "1c2f6320-7972-4621-a0f4-857d00dd2bbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 262, 262, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1/conv (Conv2D)             (None, 128, 128, 64) 9408        zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1/bn (BatchNormalization)   (None, 128, 128, 64) 256         conv1/conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1/relu (Activation)         (None, 128, 128, 64) 0           conv1/bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_2 (ZeroPadding2D (None, 130, 130, 64) 0           conv1/relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1 (MaxPooling2D)            (None, 64, 64, 64)   0           zero_padding2d_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 64, 64, 64)   256         pool1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_relu (Activation (None, 64, 64, 64)   0           conv2_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 64, 64, 128)  8192        conv2_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 64, 64, 128)  512         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 64, 64, 128)  0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_concat (Concatenat (None, 64, 64, 96)   0           pool1[0][0]                      \n",
            "                                                                 conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_bn (BatchNormali (None, 64, 64, 96)   384         conv2_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_relu (Activation (None, 64, 64, 96)   0           conv2_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 64, 64, 128)  12288       conv2_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 64, 64, 128)  512         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 64, 64, 128)  0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_concat (Concatenat (None, 64, 64, 128)  0           conv2_block1_concat[0][0]        \n",
            "                                                                 conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_bn (BatchNormali (None, 64, 64, 128)  512         conv2_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_relu (Activation (None, 64, 64, 128)  0           conv2_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 64, 64, 128)  16384       conv2_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 64, 64, 128)  512         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 64, 64, 128)  0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_concat (Concatenat (None, 64, 64, 160)  0           conv2_block2_concat[0][0]        \n",
            "                                                                 conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_bn (BatchNormali (None, 64, 64, 160)  640         conv2_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_relu (Activation (None, 64, 64, 160)  0           conv2_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_conv (Conv2D)    (None, 64, 64, 128)  20480       conv2_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_bn (BatchNormali (None, 64, 64, 128)  512         conv2_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_relu (Activation (None, 64, 64, 128)  0           conv2_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv2_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_concat (Concatenat (None, 64, 64, 192)  0           conv2_block3_concat[0][0]        \n",
            "                                                                 conv2_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_0_bn (BatchNormali (None, 64, 64, 192)  768         conv2_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_0_relu (Activation (None, 64, 64, 192)  0           conv2_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_conv (Conv2D)    (None, 64, 64, 128)  24576       conv2_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_bn (BatchNormali (None, 64, 64, 128)  512         conv2_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_relu (Activation (None, 64, 64, 128)  0           conv2_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv2_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_concat (Concatenat (None, 64, 64, 224)  0           conv2_block4_concat[0][0]        \n",
            "                                                                 conv2_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_0_bn (BatchNormali (None, 64, 64, 224)  896         conv2_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_0_relu (Activation (None, 64, 64, 224)  0           conv2_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_conv (Conv2D)    (None, 64, 64, 128)  28672       conv2_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_bn (BatchNormali (None, 64, 64, 128)  512         conv2_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_relu (Activation (None, 64, 64, 128)  0           conv2_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv2_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_concat (Concatenat (None, 64, 64, 256)  0           conv2_block5_concat[0][0]        \n",
            "                                                                 conv2_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool2_bn (BatchNormalization)   (None, 64, 64, 256)  1024        conv2_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool2_relu (Activation)         (None, 64, 64, 256)  0           pool2_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool2_conv (Conv2D)             (None, 64, 64, 128)  32768       pool2_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool2_pool (AveragePooling2D)   (None, 32, 32, 128)  0           pool2_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 32, 32, 128)  512         pool2_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_relu (Activation (None, 32, 32, 128)  0           conv3_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  16384       conv3_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_concat (Concatenat (None, 32, 32, 160)  0           pool2_pool[0][0]                 \n",
            "                                                                 conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_bn (BatchNormali (None, 32, 32, 160)  640         conv3_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_relu (Activation (None, 32, 32, 160)  0           conv3_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  20480       conv3_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_concat (Concatenat (None, 32, 32, 192)  0           conv3_block1_concat[0][0]        \n",
            "                                                                 conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_bn (BatchNormali (None, 32, 32, 192)  768         conv3_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_relu (Activation (None, 32, 32, 192)  0           conv3_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  24576       conv3_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_concat (Concatenat (None, 32, 32, 224)  0           conv3_block2_concat[0][0]        \n",
            "                                                                 conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_bn (BatchNormali (None, 32, 32, 224)  896         conv3_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_relu (Activation (None, 32, 32, 224)  0           conv3_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  28672       conv3_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_concat (Concatenat (None, 32, 32, 256)  0           conv3_block3_concat[0][0]        \n",
            "                                                                 conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_bn (BatchNormali (None, 32, 32, 256)  1024        conv3_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_relu (Activation (None, 32, 32, 256)  0           conv3_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_conv (Conv2D)    (None, 32, 32, 128)  32768       conv3_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_relu (Activation (None, 32, 32, 128)  0           conv3_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv3_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_concat (Concatenat (None, 32, 32, 288)  0           conv3_block4_concat[0][0]        \n",
            "                                                                 conv3_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_bn (BatchNormali (None, 32, 32, 288)  1152        conv3_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_relu (Activation (None, 32, 32, 288)  0           conv3_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_conv (Conv2D)    (None, 32, 32, 128)  36864       conv3_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_relu (Activation (None, 32, 32, 128)  0           conv3_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv3_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_concat (Concatenat (None, 32, 32, 320)  0           conv3_block5_concat[0][0]        \n",
            "                                                                 conv3_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_bn (BatchNormali (None, 32, 32, 320)  1280        conv3_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_relu (Activation (None, 32, 32, 320)  0           conv3_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_conv (Conv2D)    (None, 32, 32, 128)  40960       conv3_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_relu (Activation (None, 32, 32, 128)  0           conv3_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv3_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_concat (Concatenat (None, 32, 32, 352)  0           conv3_block6_concat[0][0]        \n",
            "                                                                 conv3_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_bn (BatchNormali (None, 32, 32, 352)  1408        conv3_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_relu (Activation (None, 32, 32, 352)  0           conv3_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_conv (Conv2D)    (None, 32, 32, 128)  45056       conv3_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_relu (Activation (None, 32, 32, 128)  0           conv3_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv3_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_concat (Concatenat (None, 32, 32, 384)  0           conv3_block7_concat[0][0]        \n",
            "                                                                 conv3_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_0_bn (BatchNormali (None, 32, 32, 384)  1536        conv3_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_0_relu (Activation (None, 32, 32, 384)  0           conv3_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_conv (Conv2D)    (None, 32, 32, 128)  49152       conv3_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_relu (Activation (None, 32, 32, 128)  0           conv3_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv3_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_concat (Concatenat (None, 32, 32, 416)  0           conv3_block8_concat[0][0]        \n",
            "                                                                 conv3_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_0_bn (BatchNormal (None, 32, 32, 416)  1664        conv3_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_0_relu (Activatio (None, 32, 32, 416)  0           conv3_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_conv (Conv2D)   (None, 32, 32, 128)  53248       conv3_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_bn (BatchNormal (None, 32, 32, 128)  512         conv3_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_relu (Activatio (None, 32, 32, 128)  0           conv3_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv3_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_concat (Concatena (None, 32, 32, 448)  0           conv3_block9_concat[0][0]        \n",
            "                                                                 conv3_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_0_bn (BatchNormal (None, 32, 32, 448)  1792        conv3_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_0_relu (Activatio (None, 32, 32, 448)  0           conv3_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_conv (Conv2D)   (None, 32, 32, 128)  57344       conv3_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_bn (BatchNormal (None, 32, 32, 128)  512         conv3_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_relu (Activatio (None, 32, 32, 128)  0           conv3_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv3_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_concat (Concatena (None, 32, 32, 480)  0           conv3_block10_concat[0][0]       \n",
            "                                                                 conv3_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_0_bn (BatchNormal (None, 32, 32, 480)  1920        conv3_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_0_relu (Activatio (None, 32, 32, 480)  0           conv3_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_conv (Conv2D)   (None, 32, 32, 128)  61440       conv3_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_bn (BatchNormal (None, 32, 32, 128)  512         conv3_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_relu (Activatio (None, 32, 32, 128)  0           conv3_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv3_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_concat (Concatena (None, 32, 32, 512)  0           conv3_block11_concat[0][0]       \n",
            "                                                                 conv3_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool3_bn (BatchNormalization)   (None, 32, 32, 512)  2048        conv3_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool3_relu (Activation)         (None, 32, 32, 512)  0           pool3_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool3_conv (Conv2D)             (None, 32, 32, 256)  131072      pool3_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool3_pool (AveragePooling2D)   (None, 16, 16, 256)  0           pool3_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 16, 16, 256)  1024        pool3_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_relu (Activation (None, 16, 16, 256)  0           conv4_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 128)  32768       conv4_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 16, 16, 128)  512         conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 16, 16, 128)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_concat (Concatenat (None, 16, 16, 288)  0           pool3_pool[0][0]                 \n",
            "                                                                 conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_bn (BatchNormali (None, 16, 16, 288)  1152        conv4_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_relu (Activation (None, 16, 16, 288)  0           conv4_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 128)  36864       conv4_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 16, 16, 128)  512         conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 16, 16, 128)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_concat (Concatenat (None, 16, 16, 320)  0           conv4_block1_concat[0][0]        \n",
            "                                                                 conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_bn (BatchNormali (None, 16, 16, 320)  1280        conv4_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_relu (Activation (None, 16, 16, 320)  0           conv4_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 128)  40960       conv4_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 16, 16, 128)  512         conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 16, 16, 128)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_concat (Concatenat (None, 16, 16, 352)  0           conv4_block2_concat[0][0]        \n",
            "                                                                 conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_bn (BatchNormali (None, 16, 16, 352)  1408        conv4_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_relu (Activation (None, 16, 16, 352)  0           conv4_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 128)  45056       conv4_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 16, 16, 128)  512         conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 16, 16, 128)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_concat (Concatenat (None, 16, 16, 384)  0           conv4_block3_concat[0][0]        \n",
            "                                                                 conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_bn (BatchNormali (None, 16, 16, 384)  1536        conv4_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_relu (Activation (None, 16, 16, 384)  0           conv4_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 128)  49152       conv4_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 16, 16, 128)  512         conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 16, 16, 128)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_concat (Concatenat (None, 16, 16, 416)  0           conv4_block4_concat[0][0]        \n",
            "                                                                 conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_bn (BatchNormali (None, 16, 16, 416)  1664        conv4_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_relu (Activation (None, 16, 16, 416)  0           conv4_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 128)  53248       conv4_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 16, 16, 128)  512         conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 16, 16, 128)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_concat (Concatenat (None, 16, 16, 448)  0           conv4_block5_concat[0][0]        \n",
            "                                                                 conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_bn (BatchNormali (None, 16, 16, 448)  1792        conv4_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_relu (Activation (None, 16, 16, 448)  0           conv4_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_conv (Conv2D)    (None, 16, 16, 128)  57344       conv4_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_bn (BatchNormali (None, 16, 16, 128)  512         conv4_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_relu (Activation (None, 16, 16, 128)  0           conv4_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv4_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_concat (Concatenat (None, 16, 16, 480)  0           conv4_block6_concat[0][0]        \n",
            "                                                                 conv4_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_bn (BatchNormali (None, 16, 16, 480)  1920        conv4_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_relu (Activation (None, 16, 16, 480)  0           conv4_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_conv (Conv2D)    (None, 16, 16, 128)  61440       conv4_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_bn (BatchNormali (None, 16, 16, 128)  512         conv4_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_relu (Activation (None, 16, 16, 128)  0           conv4_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv4_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_concat (Concatenat (None, 16, 16, 512)  0           conv4_block7_concat[0][0]        \n",
            "                                                                 conv4_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_bn (BatchNormali (None, 16, 16, 512)  2048        conv4_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_relu (Activation (None, 16, 16, 512)  0           conv4_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_conv (Conv2D)    (None, 16, 16, 128)  65536       conv4_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_bn (BatchNormali (None, 16, 16, 128)  512         conv4_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_relu (Activation (None, 16, 16, 128)  0           conv4_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv4_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_concat (Concatenat (None, 16, 16, 544)  0           conv4_block8_concat[0][0]        \n",
            "                                                                 conv4_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_bn (BatchNormal (None, 16, 16, 544)  2176        conv4_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_relu (Activatio (None, 16, 16, 544)  0           conv4_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_conv (Conv2D)   (None, 16, 16, 128)  69632       conv4_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_bn (BatchNormal (None, 16, 16, 128)  512         conv4_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_relu (Activatio (None, 16, 16, 128)  0           conv4_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv4_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_concat (Concatena (None, 16, 16, 576)  0           conv4_block9_concat[0][0]        \n",
            "                                                                 conv4_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_bn (BatchNormal (None, 16, 16, 576)  2304        conv4_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_relu (Activatio (None, 16, 16, 576)  0           conv4_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_conv (Conv2D)   (None, 16, 16, 128)  73728       conv4_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_bn (BatchNormal (None, 16, 16, 128)  512         conv4_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_relu (Activatio (None, 16, 16, 128)  0           conv4_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv4_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_concat (Concatena (None, 16, 16, 608)  0           conv4_block10_concat[0][0]       \n",
            "                                                                 conv4_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_bn (BatchNormal (None, 16, 16, 608)  2432        conv4_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_relu (Activatio (None, 16, 16, 608)  0           conv4_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_conv (Conv2D)   (None, 16, 16, 128)  77824       conv4_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_bn (BatchNormal (None, 16, 16, 128)  512         conv4_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_relu (Activatio (None, 16, 16, 128)  0           conv4_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv4_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_concat (Concatena (None, 16, 16, 640)  0           conv4_block11_concat[0][0]       \n",
            "                                                                 conv4_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_bn (BatchNormal (None, 16, 16, 640)  2560        conv4_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_relu (Activatio (None, 16, 16, 640)  0           conv4_block13_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_conv (Conv2D)   (None, 16, 16, 128)  81920       conv4_block13_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_bn (BatchNormal (None, 16, 16, 128)  512         conv4_block13_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_relu (Activatio (None, 16, 16, 128)  0           conv4_block13_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv4_block13_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_concat (Concatena (None, 16, 16, 672)  0           conv4_block12_concat[0][0]       \n",
            "                                                                 conv4_block13_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_bn (BatchNormal (None, 16, 16, 672)  2688        conv4_block13_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_relu (Activatio (None, 16, 16, 672)  0           conv4_block14_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_conv (Conv2D)   (None, 16, 16, 128)  86016       conv4_block14_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_bn (BatchNormal (None, 16, 16, 128)  512         conv4_block14_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_relu (Activatio (None, 16, 16, 128)  0           conv4_block14_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv4_block14_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_concat (Concatena (None, 16, 16, 704)  0           conv4_block13_concat[0][0]       \n",
            "                                                                 conv4_block14_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_bn (BatchNormal (None, 16, 16, 704)  2816        conv4_block14_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_relu (Activatio (None, 16, 16, 704)  0           conv4_block15_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_conv (Conv2D)   (None, 16, 16, 128)  90112       conv4_block15_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_bn (BatchNormal (None, 16, 16, 128)  512         conv4_block15_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_relu (Activatio (None, 16, 16, 128)  0           conv4_block15_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv4_block15_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_concat (Concatena (None, 16, 16, 736)  0           conv4_block14_concat[0][0]       \n",
            "                                                                 conv4_block15_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_bn (BatchNormal (None, 16, 16, 736)  2944        conv4_block15_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_relu (Activatio (None, 16, 16, 736)  0           conv4_block16_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_conv (Conv2D)   (None, 16, 16, 128)  94208       conv4_block16_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_bn (BatchNormal (None, 16, 16, 128)  512         conv4_block16_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_relu (Activatio (None, 16, 16, 128)  0           conv4_block16_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv4_block16_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_concat (Concatena (None, 16, 16, 768)  0           conv4_block15_concat[0][0]       \n",
            "                                                                 conv4_block16_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_0_bn (BatchNormal (None, 16, 16, 768)  3072        conv4_block16_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_0_relu (Activatio (None, 16, 16, 768)  0           conv4_block17_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_conv (Conv2D)   (None, 16, 16, 128)  98304       conv4_block17_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_bn (BatchNormal (None, 16, 16, 128)  512         conv4_block17_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_relu (Activatio (None, 16, 16, 128)  0           conv4_block17_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv4_block17_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_concat (Concatena (None, 16, 16, 800)  0           conv4_block16_concat[0][0]       \n",
            "                                                                 conv4_block17_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_0_bn (BatchNormal (None, 16, 16, 800)  3200        conv4_block17_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_0_relu (Activatio (None, 16, 16, 800)  0           conv4_block18_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_conv (Conv2D)   (None, 16, 16, 128)  102400      conv4_block18_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_bn (BatchNormal (None, 16, 16, 128)  512         conv4_block18_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_relu (Activatio (None, 16, 16, 128)  0           conv4_block18_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv4_block18_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_concat (Concatena (None, 16, 16, 832)  0           conv4_block17_concat[0][0]       \n",
            "                                                                 conv4_block18_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_0_bn (BatchNormal (None, 16, 16, 832)  3328        conv4_block18_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_0_relu (Activatio (None, 16, 16, 832)  0           conv4_block19_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_conv (Conv2D)   (None, 16, 16, 128)  106496      conv4_block19_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_bn (BatchNormal (None, 16, 16, 128)  512         conv4_block19_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_relu (Activatio (None, 16, 16, 128)  0           conv4_block19_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv4_block19_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_concat (Concatena (None, 16, 16, 864)  0           conv4_block18_concat[0][0]       \n",
            "                                                                 conv4_block19_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_0_bn (BatchNormal (None, 16, 16, 864)  3456        conv4_block19_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_0_relu (Activatio (None, 16, 16, 864)  0           conv4_block20_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_conv (Conv2D)   (None, 16, 16, 128)  110592      conv4_block20_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_bn (BatchNormal (None, 16, 16, 128)  512         conv4_block20_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_relu (Activatio (None, 16, 16, 128)  0           conv4_block20_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv4_block20_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_concat (Concatena (None, 16, 16, 896)  0           conv4_block19_concat[0][0]       \n",
            "                                                                 conv4_block20_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_0_bn (BatchNormal (None, 16, 16, 896)  3584        conv4_block20_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_0_relu (Activatio (None, 16, 16, 896)  0           conv4_block21_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_conv (Conv2D)   (None, 16, 16, 128)  114688      conv4_block21_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_bn (BatchNormal (None, 16, 16, 128)  512         conv4_block21_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_relu (Activatio (None, 16, 16, 128)  0           conv4_block21_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv4_block21_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_concat (Concatena (None, 16, 16, 928)  0           conv4_block20_concat[0][0]       \n",
            "                                                                 conv4_block21_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_0_bn (BatchNormal (None, 16, 16, 928)  3712        conv4_block21_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_0_relu (Activatio (None, 16, 16, 928)  0           conv4_block22_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_conv (Conv2D)   (None, 16, 16, 128)  118784      conv4_block22_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_bn (BatchNormal (None, 16, 16, 128)  512         conv4_block22_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_relu (Activatio (None, 16, 16, 128)  0           conv4_block22_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv4_block22_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_concat (Concatena (None, 16, 16, 960)  0           conv4_block21_concat[0][0]       \n",
            "                                                                 conv4_block22_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_0_bn (BatchNormal (None, 16, 16, 960)  3840        conv4_block22_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_0_relu (Activatio (None, 16, 16, 960)  0           conv4_block23_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_conv (Conv2D)   (None, 16, 16, 128)  122880      conv4_block23_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_bn (BatchNormal (None, 16, 16, 128)  512         conv4_block23_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_relu (Activatio (None, 16, 16, 128)  0           conv4_block23_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv4_block23_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_concat (Concatena (None, 16, 16, 992)  0           conv4_block22_concat[0][0]       \n",
            "                                                                 conv4_block23_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_0_bn (BatchNormal (None, 16, 16, 992)  3968        conv4_block23_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_0_relu (Activatio (None, 16, 16, 992)  0           conv4_block24_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_conv (Conv2D)   (None, 16, 16, 128)  126976      conv4_block24_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_bn (BatchNormal (None, 16, 16, 128)  512         conv4_block24_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_relu (Activatio (None, 16, 16, 128)  0           conv4_block24_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv4_block24_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_concat (Concatena (None, 16, 16, 1024) 0           conv4_block23_concat[0][0]       \n",
            "                                                                 conv4_block24_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool4_bn (BatchNormalization)   (None, 16, 16, 1024) 4096        conv4_block24_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool4_relu (Activation)         (None, 16, 16, 1024) 0           pool4_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool4_conv (Conv2D)             (None, 16, 16, 512)  524288      pool4_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool4_pool (AveragePooling2D)   (None, 8, 8, 512)    0           pool4_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 8, 8, 512)    2048        pool4_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_relu (Activation (None, 8, 8, 512)    0           conv5_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 128)    65536       conv5_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 8, 8, 128)    512         conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 8, 8, 128)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 32)     36864       conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_concat (Concatenat (None, 8, 8, 544)    0           pool4_pool[0][0]                 \n",
            "                                                                 conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_0_bn (BatchNormali (None, 8, 8, 544)    2176        conv5_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_0_relu (Activation (None, 8, 8, 544)    0           conv5_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 128)    69632       conv5_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 8, 8, 128)    512         conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 8, 8, 128)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 32)     36864       conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_concat (Concatenat (None, 8, 8, 576)    0           conv5_block1_concat[0][0]        \n",
            "                                                                 conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_0_bn (BatchNormali (None, 8, 8, 576)    2304        conv5_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_0_relu (Activation (None, 8, 8, 576)    0           conv5_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 128)    73728       conv5_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 8, 8, 128)    512         conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 8, 8, 128)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 32)     36864       conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_concat (Concatenat (None, 8, 8, 608)    0           conv5_block2_concat[0][0]        \n",
            "                                                                 conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_0_bn (BatchNormali (None, 8, 8, 608)    2432        conv5_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_0_relu (Activation (None, 8, 8, 608)    0           conv5_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_conv (Conv2D)    (None, 8, 8, 128)    77824       conv5_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_bn (BatchNormali (None, 8, 8, 128)    512         conv5_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_relu (Activation (None, 8, 8, 128)    0           conv5_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_2_conv (Conv2D)    (None, 8, 8, 32)     36864       conv5_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_concat (Concatenat (None, 8, 8, 640)    0           conv5_block3_concat[0][0]        \n",
            "                                                                 conv5_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_0_bn (BatchNormali (None, 8, 8, 640)    2560        conv5_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_0_relu (Activation (None, 8, 8, 640)    0           conv5_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_conv (Conv2D)    (None, 8, 8, 128)    81920       conv5_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_bn (BatchNormali (None, 8, 8, 128)    512         conv5_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_relu (Activation (None, 8, 8, 128)    0           conv5_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_2_conv (Conv2D)    (None, 8, 8, 32)     36864       conv5_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_concat (Concatenat (None, 8, 8, 672)    0           conv5_block4_concat[0][0]        \n",
            "                                                                 conv5_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_0_bn (BatchNormali (None, 8, 8, 672)    2688        conv5_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_0_relu (Activation (None, 8, 8, 672)    0           conv5_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_conv (Conv2D)    (None, 8, 8, 128)    86016       conv5_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_bn (BatchNormali (None, 8, 8, 128)    512         conv5_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_relu (Activation (None, 8, 8, 128)    0           conv5_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_2_conv (Conv2D)    (None, 8, 8, 32)     36864       conv5_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_concat (Concatenat (None, 8, 8, 704)    0           conv5_block5_concat[0][0]        \n",
            "                                                                 conv5_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_0_bn (BatchNormali (None, 8, 8, 704)    2816        conv5_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_0_relu (Activation (None, 8, 8, 704)    0           conv5_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_conv (Conv2D)    (None, 8, 8, 128)    90112       conv5_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_bn (BatchNormali (None, 8, 8, 128)    512         conv5_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_relu (Activation (None, 8, 8, 128)    0           conv5_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_2_conv (Conv2D)    (None, 8, 8, 32)     36864       conv5_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_concat (Concatenat (None, 8, 8, 736)    0           conv5_block6_concat[0][0]        \n",
            "                                                                 conv5_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_0_bn (BatchNormali (None, 8, 8, 736)    2944        conv5_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_0_relu (Activation (None, 8, 8, 736)    0           conv5_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_conv (Conv2D)    (None, 8, 8, 128)    94208       conv5_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_bn (BatchNormali (None, 8, 8, 128)    512         conv5_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_relu (Activation (None, 8, 8, 128)    0           conv5_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_2_conv (Conv2D)    (None, 8, 8, 32)     36864       conv5_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_concat (Concatenat (None, 8, 8, 768)    0           conv5_block7_concat[0][0]        \n",
            "                                                                 conv5_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_0_bn (BatchNormali (None, 8, 8, 768)    3072        conv5_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_0_relu (Activation (None, 8, 8, 768)    0           conv5_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_conv (Conv2D)    (None, 8, 8, 128)    98304       conv5_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_bn (BatchNormali (None, 8, 8, 128)    512         conv5_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_relu (Activation (None, 8, 8, 128)    0           conv5_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_2_conv (Conv2D)    (None, 8, 8, 32)     36864       conv5_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_concat (Concatenat (None, 8, 8, 800)    0           conv5_block8_concat[0][0]        \n",
            "                                                                 conv5_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_0_bn (BatchNormal (None, 8, 8, 800)    3200        conv5_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_0_relu (Activatio (None, 8, 8, 800)    0           conv5_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_conv (Conv2D)   (None, 8, 8, 128)    102400      conv5_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_bn (BatchNormal (None, 8, 8, 128)    512         conv5_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_relu (Activatio (None, 8, 8, 128)    0           conv5_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_2_conv (Conv2D)   (None, 8, 8, 32)     36864       conv5_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_concat (Concatena (None, 8, 8, 832)    0           conv5_block9_concat[0][0]        \n",
            "                                                                 conv5_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_0_bn (BatchNormal (None, 8, 8, 832)    3328        conv5_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_0_relu (Activatio (None, 8, 8, 832)    0           conv5_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_conv (Conv2D)   (None, 8, 8, 128)    106496      conv5_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_bn (BatchNormal (None, 8, 8, 128)    512         conv5_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_relu (Activatio (None, 8, 8, 128)    0           conv5_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_2_conv (Conv2D)   (None, 8, 8, 32)     36864       conv5_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_concat (Concatena (None, 8, 8, 864)    0           conv5_block10_concat[0][0]       \n",
            "                                                                 conv5_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_0_bn (BatchNormal (None, 8, 8, 864)    3456        conv5_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_0_relu (Activatio (None, 8, 8, 864)    0           conv5_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_conv (Conv2D)   (None, 8, 8, 128)    110592      conv5_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_bn (BatchNormal (None, 8, 8, 128)    512         conv5_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_relu (Activatio (None, 8, 8, 128)    0           conv5_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_2_conv (Conv2D)   (None, 8, 8, 32)     36864       conv5_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_concat (Concatena (None, 8, 8, 896)    0           conv5_block11_concat[0][0]       \n",
            "                                                                 conv5_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_0_bn (BatchNormal (None, 8, 8, 896)    3584        conv5_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_0_relu (Activatio (None, 8, 8, 896)    0           conv5_block13_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_conv (Conv2D)   (None, 8, 8, 128)    114688      conv5_block13_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_bn (BatchNormal (None, 8, 8, 128)    512         conv5_block13_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_relu (Activatio (None, 8, 8, 128)    0           conv5_block13_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_2_conv (Conv2D)   (None, 8, 8, 32)     36864       conv5_block13_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_concat (Concatena (None, 8, 8, 928)    0           conv5_block12_concat[0][0]       \n",
            "                                                                 conv5_block13_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_0_bn (BatchNormal (None, 8, 8, 928)    3712        conv5_block13_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_0_relu (Activatio (None, 8, 8, 928)    0           conv5_block14_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_conv (Conv2D)   (None, 8, 8, 128)    118784      conv5_block14_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_bn (BatchNormal (None, 8, 8, 128)    512         conv5_block14_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_relu (Activatio (None, 8, 8, 128)    0           conv5_block14_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_2_conv (Conv2D)   (None, 8, 8, 32)     36864       conv5_block14_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_concat (Concatena (None, 8, 8, 960)    0           conv5_block13_concat[0][0]       \n",
            "                                                                 conv5_block14_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_0_bn (BatchNormal (None, 8, 8, 960)    3840        conv5_block14_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_0_relu (Activatio (None, 8, 8, 960)    0           conv5_block15_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_conv (Conv2D)   (None, 8, 8, 128)    122880      conv5_block15_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_bn (BatchNormal (None, 8, 8, 128)    512         conv5_block15_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_relu (Activatio (None, 8, 8, 128)    0           conv5_block15_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_2_conv (Conv2D)   (None, 8, 8, 32)     36864       conv5_block15_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_concat (Concatena (None, 8, 8, 992)    0           conv5_block14_concat[0][0]       \n",
            "                                                                 conv5_block15_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_0_bn (BatchNormal (None, 8, 8, 992)    3968        conv5_block15_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_0_relu (Activatio (None, 8, 8, 992)    0           conv5_block16_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_conv (Conv2D)   (None, 8, 8, 128)    126976      conv5_block16_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_bn (BatchNormal (None, 8, 8, 128)    512         conv5_block16_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_relu (Activatio (None, 8, 8, 128)    0           conv5_block16_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_2_conv (Conv2D)   (None, 8, 8, 32)     36864       conv5_block16_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_concat (Concatena (None, 8, 8, 1024)   0           conv5_block15_concat[0][0]       \n",
            "                                                                 conv5_block16_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bn (BatchNormalization)         (None, 8, 8, 1024)   4096        conv5_block16_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "relu (Activation)               (None, 8, 8, 1024)   0           bn[0][0]                         \n",
            "__________________________________________________________________________________________________\n",
            "max_pool (GlobalMaxPooling2D)   (None, 1024)         0           relu[0][0]                       \n",
            "==================================================================================================\n",
            "Total params: 7,037,504\n",
            "Trainable params: 6,953,856\n",
            "Non-trainable params: 83,648\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XdE0Fd3IAzm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = model.get_output_at(-1)\n",
        "#layer_output = self.layer.get_output_at(-1)\n",
        "predictions = Dense(14, activation=\"sigmoid\",name=\"predictions\")(x)\n",
        "model11 = Model(inputs=img_in, outputs=predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jijGU8dbfp8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for layer in model11.layers[:-3]:\n",
        "#    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHZ3aKGykT7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in model11.layers[:-5]:\n",
        "   layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QizpUWFjL7Fd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model11.load_weights('/content/drive/My Drive/Capstone/PretrainedWeights/brucechou1983_CheXNet_Keras_0.3.0_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AXEKZ_82kO0",
        "colab_type": "code",
        "outputId": "29d38c08-33d2-44ad-c9d7-36a53ac4dfbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# Create the model\n",
        "model_pretrainedW_3000= Sequential()\n",
        " \n",
        "# Add the vgg convolutional base model\n",
        "model_pretrainedW_3000.add(model11)\n",
        " \n",
        "# Add new layers\n",
        "# model_pretrainedW_3000.add(Dropout(0.25))\n",
        "# model_pretrainedW_3000.add(Dense(256, activation='relu'))\n",
        "# model_pretrainedW_3000.add(Dropout(0.25))\n",
        "model_pretrainedW_3000.add(Dense(512, activation='relu'))\n",
        "model_pretrainedW_3000.add(Dropout(0.25))\n",
        "model_pretrainedW_3000.add(Dense(256, activation='relu'))\n",
        "model_pretrainedW_3000.add(Dropout(0.25))\n",
        "#model_pretrainedW_3000.add(Dense(32, activation='relu'))\n",
        "#model_pretrainedW_3000.add(Dropout(0.25))\n",
        "model_pretrainedW_3000.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#model_vgg_flow.add(Dense(1, activation='softmax')) \n",
        "# Show a summary of the model. Check the number of trainable parameters\n",
        "model_pretrainedW_3000.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_1 (Model)              (None, 14)                7051854   \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 512)               7680      \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 32)                8224      \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 7,199,119\n",
            "Trainable params: 161,615\n",
            "Non-trainable params: 7,037,504\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMS-4GxS2npx",
        "colab_type": "code",
        "outputId": "812a9d5c-9934-4b69-f84b-c62d7ce05dd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size=128\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "adam = optimizers.Adam(lr=0.1, decay=1e-1, beta_1=0.333, beta_2=0.111 )\n",
        "#model_pretrainedW_3000.compile(optimizer=adam,loss='binary_crossentropy',metrics=['accuracy'])\n",
        "checkpointer = ModelCheckpoint(filepath='/content/drive/My Drive/Capstone/chexnet_weights_pnemonia_adam.h5' , monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='val_acc')\n",
        "model_pretrainedW_3000.compile(optimizer= adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_pretrainedW_3000.fit_generator(\n",
        "        train_generator_1,\n",
        "        steps_per_epoch=4838 // batch_size,\n",
        "        epochs=70,\n",
        "        validation_data=validation_generator_1, callbacks =[checkpointer],\n",
        "        validation_steps=441// batch_size)\n",
        "#model_pretrainedW_3000.save_weights('/content/drive/My Drive/Capstone/chexnet_weights_pnemonia_adam.h5')  # always save your weights after training or during training\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:407: RuntimeWarning: ModelCheckpoint mode val_acc is unknown, fallback to auto mode.\n",
            "  RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "37/37 [==============================] - 35s 952ms/step - loss: 0.6609 - acc: 0.6284 - val_loss: 0.6870 - val_acc: 0.5730\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.68700, saving model to /content/drive/My Drive/Capstone/chexnet_weights_pnemonia_adam.h5\n",
            "Epoch 2/70\n",
            "37/37 [==============================] - 15s 415ms/step - loss: 0.6634 - acc: 0.6225 - val_loss: 0.6775 - val_acc: 0.5938\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.68700 to 0.67754, saving model to /content/drive/My Drive/Capstone/chexnet_weights_pnemonia_adam.h5\n",
            "Epoch 3/70\n",
            "37/37 [==============================] - 18s 494ms/step - loss: 0.6662 - acc: 0.6166 - val_loss: 0.6715 - val_acc: 0.6042\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.67754 to 0.67153, saving model to /content/drive/My Drive/Capstone/chexnet_weights_pnemonia_adam.h5\n",
            "Epoch 4/70\n",
            "37/37 [==============================] - 18s 483ms/step - loss: 0.6761 - acc: 0.5945 - val_loss: 0.6760 - val_acc: 0.5938\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.67153\n",
            "Epoch 5/70\n",
            "37/37 [==============================] - 18s 490ms/step - loss: 0.6668 - acc: 0.6149 - val_loss: 0.7412 - val_acc: 0.4583\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.67153\n",
            "Epoch 6/70\n",
            "37/37 [==============================] - 18s 486ms/step - loss: 0.6707 - acc: 0.6064 - val_loss: 0.6597 - val_acc: 0.6292\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.67153 to 0.65975, saving model to /content/drive/My Drive/Capstone/chexnet_weights_pnemonia_adam.h5\n",
            "Epoch 7/70\n",
            "37/37 [==============================] - 18s 490ms/step - loss: 0.6540 - acc: 0.6410 - val_loss: 0.6977 - val_acc: 0.5521\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.65975\n",
            "Epoch 8/70\n",
            "37/37 [==============================] - 18s 479ms/step - loss: 0.6612 - acc: 0.6263 - val_loss: 0.6883 - val_acc: 0.5729\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.65975\n",
            "Epoch 9/70\n",
            "37/37 [==============================] - 18s 489ms/step - loss: 0.6586 - acc: 0.6309 - val_loss: 0.6996 - val_acc: 0.5521\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.65975\n",
            "Epoch 10/70\n",
            "37/37 [==============================] - 18s 491ms/step - loss: 0.6750 - acc: 0.5988 - val_loss: 0.7099 - val_acc: 0.5281\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.65975\n",
            "Epoch 11/70\n",
            "37/37 [==============================] - 18s 496ms/step - loss: 0.6608 - acc: 0.6267 - val_loss: 0.7089 - val_acc: 0.5312\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.65975\n",
            "Epoch 12/70\n",
            "37/37 [==============================] - 18s 495ms/step - loss: 0.6638 - acc: 0.6208 - val_loss: 0.7200 - val_acc: 0.5104\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.65975\n",
            "Epoch 13/70\n",
            "37/37 [==============================] - 18s 487ms/step - loss: 0.6577 - acc: 0.6326 - val_loss: 0.6453 - val_acc: 0.6562\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.65975 to 0.64534, saving model to /content/drive/My Drive/Capstone/chexnet_weights_pnemonia_adam.h5\n",
            "Epoch 14/70\n",
            "37/37 [==============================] - 18s 496ms/step - loss: 0.6681 - acc: 0.6123 - val_loss: 0.6882 - val_acc: 0.5729\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.64534\n",
            "Epoch 15/70\n",
            "37/37 [==============================] - 18s 485ms/step - loss: 0.6616 - acc: 0.6250 - val_loss: 0.6824 - val_acc: 0.5843\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.64534\n",
            "Epoch 16/70\n",
            "37/37 [==============================] - 18s 482ms/step - loss: 0.6640 - acc: 0.6204 - val_loss: 0.7304 - val_acc: 0.4896\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.64534\n",
            "Epoch 17/70\n",
            "37/37 [==============================] - 18s 486ms/step - loss: 0.6697 - acc: 0.6090 - val_loss: 0.6616 - val_acc: 0.6250\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.64534\n",
            "Epoch 18/70\n",
            "37/37 [==============================] - 18s 487ms/step - loss: 0.6655 - acc: 0.6174 - val_loss: 0.7144 - val_acc: 0.5208\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.64534\n",
            "Epoch 19/70\n",
            "37/37 [==============================] - 19s 505ms/step - loss: 0.6663 - acc: 0.6157 - val_loss: 0.6564 - val_acc: 0.6354\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.64534\n",
            "Epoch 20/70\n",
            "37/37 [==============================] - 18s 475ms/step - loss: 0.6555 - acc: 0.6371 - val_loss: 0.7174 - val_acc: 0.5169\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.64534\n",
            "Epoch 21/70\n",
            "37/37 [==============================] - 18s 494ms/step - loss: 0.6716 - acc: 0.6056 - val_loss: 0.6986 - val_acc: 0.5521\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.64534\n",
            "Epoch 22/70\n",
            "37/37 [==============================] - 18s 487ms/step - loss: 0.6616 - acc: 0.6250 - val_loss: 0.6510 - val_acc: 0.6458\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.64534\n",
            "Epoch 23/70\n",
            "37/37 [==============================] - 18s 490ms/step - loss: 0.6654 - acc: 0.6174 - val_loss: 0.7088 - val_acc: 0.5312\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.64534\n",
            "Epoch 24/70\n",
            "37/37 [==============================] - 18s 479ms/step - loss: 0.6527 - acc: 0.6425 - val_loss: 0.6939 - val_acc: 0.5618\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.64534\n",
            "Epoch 25/70\n",
            "37/37 [==============================] - 18s 495ms/step - loss: 0.6676 - acc: 0.6132 - val_loss: 0.6616 - val_acc: 0.6250\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.64534\n",
            "Epoch 26/70\n",
            "37/37 [==============================] - 18s 484ms/step - loss: 0.6590 - acc: 0.6301 - val_loss: 0.7146 - val_acc: 0.5208\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.64534\n",
            "Epoch 27/70\n",
            "37/37 [==============================] - 18s 486ms/step - loss: 0.6612 - acc: 0.6258 - val_loss: 0.6721 - val_acc: 0.6042\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.64534\n",
            "Epoch 28/70\n",
            "37/37 [==============================] - 18s 480ms/step - loss: 0.6696 - acc: 0.6090 - val_loss: 0.7136 - val_acc: 0.5208\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.64534\n",
            "Epoch 29/70\n",
            "37/37 [==============================] - 18s 487ms/step - loss: 0.6684 - acc: 0.6115 - val_loss: 0.6707 - val_acc: 0.6067\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.64534\n",
            "Epoch 30/70\n",
            "37/37 [==============================] - 18s 492ms/step - loss: 0.6540 - acc: 0.6402 - val_loss: 0.6877 - val_acc: 0.5729\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.64534\n",
            "Epoch 31/70\n",
            "37/37 [==============================] - 18s 494ms/step - loss: 0.6582 - acc: 0.6318 - val_loss: 0.6826 - val_acc: 0.5833\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.64534\n",
            "Epoch 32/70\n",
            "37/37 [==============================] - 18s 493ms/step - loss: 0.6713 - acc: 0.6056 - val_loss: 0.7391 - val_acc: 0.4688\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.64534\n",
            "Epoch 33/70\n",
            "37/37 [==============================] - 18s 478ms/step - loss: 0.6641 - acc: 0.6200 - val_loss: 0.6875 - val_acc: 0.5729\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.64534\n",
            "Epoch 34/70\n",
            "37/37 [==============================] - 18s 489ms/step - loss: 0.6700 - acc: 0.6081 - val_loss: 0.6485 - val_acc: 0.6517\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.64534\n",
            "Epoch 35/70\n",
            "37/37 [==============================] - 18s 486ms/step - loss: 0.6695 - acc: 0.6090 - val_loss: 0.6667 - val_acc: 0.6146\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.64534\n",
            "Epoch 36/70\n",
            "37/37 [==============================] - 18s 485ms/step - loss: 0.6600 - acc: 0.6284 - val_loss: 0.7076 - val_acc: 0.5312\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.64534\n",
            "Epoch 37/70\n",
            "37/37 [==============================] - 18s 477ms/step - loss: 0.6702 - acc: 0.6077 - val_loss: 0.7125 - val_acc: 0.5208\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.64534\n",
            "Epoch 38/70\n",
            "37/37 [==============================] - 18s 494ms/step - loss: 0.6637 - acc: 0.6208 - val_loss: 0.6982 - val_acc: 0.5506\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.64534\n",
            "Epoch 39/70\n",
            "37/37 [==============================] - 18s 493ms/step - loss: 0.6600 - acc: 0.6284 - val_loss: 0.6926 - val_acc: 0.5625\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.64534\n",
            "Epoch 40/70\n",
            "37/37 [==============================] - 18s 488ms/step - loss: 0.6625 - acc: 0.6233 - val_loss: 0.6821 - val_acc: 0.5833\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.64534\n",
            "Epoch 41/70\n",
            "37/37 [==============================] - 18s 484ms/step - loss: 0.6523 - acc: 0.6438 - val_loss: 0.6980 - val_acc: 0.5521\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.64534\n",
            "Epoch 42/70\n",
            "37/37 [==============================] - 18s 495ms/step - loss: 0.6705 - acc: 0.6073 - val_loss: 0.7185 - val_acc: 0.5104\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.64534\n",
            "Epoch 43/70\n",
            "37/37 [==============================] - 18s 495ms/step - loss: 0.6650 - acc: 0.6182 - val_loss: 0.6540 - val_acc: 0.6404\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.64534\n",
            "Epoch 44/70\n",
            "37/37 [==============================] - 18s 490ms/step - loss: 0.6654 - acc: 0.6174 - val_loss: 0.6976 - val_acc: 0.5521\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.64534\n",
            "Epoch 45/70\n",
            "37/37 [==============================] - 18s 480ms/step - loss: 0.6598 - acc: 0.6286 - val_loss: 0.7133 - val_acc: 0.5208\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.64534\n",
            "Epoch 46/70\n",
            "37/37 [==============================] - 18s 492ms/step - loss: 0.6729 - acc: 0.6022 - val_loss: 0.6821 - val_acc: 0.5833\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.64534\n",
            "Epoch 47/70\n",
            "37/37 [==============================] - 18s 493ms/step - loss: 0.6549 - acc: 0.6385 - val_loss: 0.6772 - val_acc: 0.5938\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.64534\n",
            "Epoch 48/70\n",
            "37/37 [==============================] - 18s 494ms/step - loss: 0.6599 - acc: 0.6284 - val_loss: 0.6821 - val_acc: 0.5843\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.64534\n",
            "Epoch 49/70\n",
            "37/37 [==============================] - 18s 490ms/step - loss: 0.6747 - acc: 0.5989 - val_loss: 0.6926 - val_acc: 0.5625\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.64534\n",
            "Epoch 50/70\n",
            "37/37 [==============================] - 18s 489ms/step - loss: 0.6553 - acc: 0.6377 - val_loss: 0.6825 - val_acc: 0.5833\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.64534\n",
            "Epoch 51/70\n",
            "37/37 [==============================] - 18s 492ms/step - loss: 0.6650 - acc: 0.6182 - val_loss: 0.6878 - val_acc: 0.5729\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.64534\n",
            "Epoch 52/70\n",
            "37/37 [==============================] - 18s 490ms/step - loss: 0.6607 - acc: 0.6267 - val_loss: 0.7047 - val_acc: 0.5393\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.64534\n",
            "Epoch 53/70\n",
            "37/37 [==============================] - 18s 478ms/step - loss: 0.6853 - acc: 0.5776 - val_loss: 0.6667 - val_acc: 0.6146\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.64534\n",
            "Epoch 54/70\n",
            "37/37 [==============================] - 18s 487ms/step - loss: 0.6666 - acc: 0.6149 - val_loss: 0.6821 - val_acc: 0.5833\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.64534\n",
            "Epoch 55/70\n",
            "37/37 [==============================] - 18s 494ms/step - loss: 0.6649 - acc: 0.6182 - val_loss: 0.7385 - val_acc: 0.4688\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.64534\n",
            "Epoch 56/70\n",
            "37/37 [==============================] - 18s 492ms/step - loss: 0.6482 - acc: 0.6520 - val_loss: 0.6876 - val_acc: 0.5729\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.64534\n",
            "Epoch 57/70\n",
            "37/37 [==============================] - 18s 480ms/step - loss: 0.6708 - acc: 0.6065 - val_loss: 0.6707 - val_acc: 0.6067\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.64534\n",
            "Epoch 58/70\n",
            "37/37 [==============================] - 18s 487ms/step - loss: 0.6658 - acc: 0.6166 - val_loss: 0.6874 - val_acc: 0.5729\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.64534\n",
            "Epoch 59/70\n",
            "37/37 [==============================] - 18s 488ms/step - loss: 0.6641 - acc: 0.6199 - val_loss: 0.6771 - val_acc: 0.5938\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.64534\n",
            "Epoch 60/70\n",
            "37/37 [==============================] - 18s 488ms/step - loss: 0.6557 - acc: 0.6368 - val_loss: 0.7032 - val_acc: 0.5417\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.64534\n",
            "Epoch 61/70\n",
            "37/37 [==============================] - 18s 481ms/step - loss: 0.6717 - acc: 0.6048 - val_loss: 0.6771 - val_acc: 0.5938\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.64534\n",
            "Epoch 62/70\n",
            "37/37 [==============================] - 18s 489ms/step - loss: 0.6658 - acc: 0.6166 - val_loss: 0.7152 - val_acc: 0.5169\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.64534\n",
            "Epoch 63/70\n",
            "37/37 [==============================] - 18s 494ms/step - loss: 0.6696 - acc: 0.6090 - val_loss: 0.7079 - val_acc: 0.5312\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.64534\n",
            "Epoch 64/70\n",
            "37/37 [==============================] - 18s 491ms/step - loss: 0.6574 - acc: 0.6334 - val_loss: 0.7081 - val_acc: 0.5312\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.64534\n",
            "Epoch 65/70\n",
            "37/37 [==============================] - 18s 484ms/step - loss: 0.6633 - acc: 0.6217 - val_loss: 0.6303 - val_acc: 0.6875\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.64534 to 0.63034, saving model to /content/drive/My Drive/Capstone/chexnet_weights_pnemonia_adam.h5\n",
            "Epoch 66/70\n",
            "37/37 [==============================] - 19s 501ms/step - loss: 0.6713 - acc: 0.6056 - val_loss: 0.7153 - val_acc: 0.5169\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.63034\n",
            "Epoch 67/70\n",
            "37/37 [==============================] - 18s 489ms/step - loss: 0.6625 - acc: 0.6233 - val_loss: 0.6875 - val_acc: 0.5729\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.63034\n",
            "Epoch 68/70\n",
            "37/37 [==============================] - 18s 491ms/step - loss: 0.6671 - acc: 0.6140 - val_loss: 0.6928 - val_acc: 0.5625\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.63034\n",
            "Epoch 69/70\n",
            "37/37 [==============================] - 18s 493ms/step - loss: 0.6561 - acc: 0.6360 - val_loss: 0.7032 - val_acc: 0.5417\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.63034\n",
            "Epoch 70/70\n",
            "37/37 [==============================] - 18s 487ms/step - loss: 0.6682 - acc: 0.6117 - val_loss: 0.6929 - val_acc: 0.5625\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.63034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpKVIDiB3sci",
        "colab_type": "code",
        "outputId": "5ca903f4-8eee-46c0-e1ed-f4338e5ec98a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import os\n",
        "import cv2 \n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "patientID=[]\n",
        "filename=[]\n",
        "TestDir= '/content/drive/My Drive/Capstone/validation_png/'\n",
        "\n",
        "# loop over the input images\n",
        "dirs = os.listdir(TestDir) \n",
        "for dir in dirs:\n",
        "    absDirPath = os.path.join(os.path.sep,TestDir, dir)\n",
        "    images = os.listdir(absDirPath)\n",
        "    for imageFileName in images:\n",
        "        \n",
        "        # load the image, pre-process it, and store it in the data list\n",
        "        imageFullPath = os.path.join(TestDir, dir, imageFileName)\n",
        "        #print(imageFullPath)\n",
        "        img = load_img(imageFullPath)\n",
        "        arr = img_to_array(img)  #Numpy array with shape (233,233,3)\n",
        "        arr = cv2.resize(arr, (256,256)) #Numpy array with shape (HEIGHT, WIDTH,3)\n",
        "        #print(arr.shape)\n",
        "        data.append(arr)\n",
        "        patientID.append(imageFileName.split('/')[-1].split('.')[0])\n",
        "        #print(patientID)\n",
        "        filename.append(imageFullPath)\n",
        "        \n",
        "        #label = classes_to_int(dir)\n",
        "        if(dir== 'validation_pneumonia'):\n",
        "          label=1\n",
        "        else:\n",
        "          label=0\n",
        "        #print(label)\n",
        "        labels.append(label)\n",
        "    print(len(images))\n",
        "    \n",
        "print('Number of images :-',len(data))\n",
        "print('Number of Labels',len(labels))\n",
        "print('Number of patientID :-',len(patientID))\n",
        "print(len(filename))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "191\n",
            "250\n",
            "Number of images :- 441\n",
            "Number of Labels 441\n",
            "Number of patientID :- 441\n",
            "441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTC0tQ1OpfI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Valid_df = pd.DataFrame({ \n",
        "                        'patientId':patientID,\n",
        "                       'images':data,\n",
        "                        'Labels': labels,\n",
        "                         'Filenames':filename\n",
        "                         })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDN99e0spf-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model= load_model(\"/content/drive/My Drive/Capstone/model_pnemonia_adam.h5\")\n",
        "y_preds=[]\n",
        "for img in Valid_df['images']:\n",
        "  img=img.reshape(-1,256,256,3)\n",
        "  pred_1=model_pretrainedW_3000.predict(img)\n",
        "  y_preds.append(pred_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoPIuY8YprOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Valid_df['preds']=y_preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kYRZ_FzquYR",
        "colab_type": "code",
        "outputId": "fd95abea-41f1-4a00-b2ea-b60fb0dea435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "Valid_df[['Labels', 'preds']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Labels</th>\n",
              "      <th>preds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>412</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>415</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>417</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>418</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>419</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>421</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>422</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>429</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>430</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>432</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>433</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>0</td>\n",
              "      <td>[[0.37732956]]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>441 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Labels           preds\n",
              "0         1  [[0.37732956]]\n",
              "1         1  [[0.37732956]]\n",
              "2         1  [[0.37732956]]\n",
              "3         1  [[0.37732956]]\n",
              "4         1  [[0.37732956]]\n",
              "5         1  [[0.37732956]]\n",
              "6         1  [[0.37732956]]\n",
              "7         1  [[0.37732956]]\n",
              "8         1  [[0.37732956]]\n",
              "9         1  [[0.37732956]]\n",
              "10        1  [[0.37732956]]\n",
              "11        1  [[0.37732956]]\n",
              "12        1  [[0.37732956]]\n",
              "13        1  [[0.37732956]]\n",
              "14        1  [[0.37732956]]\n",
              "15        1  [[0.37732956]]\n",
              "16        1  [[0.37732956]]\n",
              "17        1  [[0.37732956]]\n",
              "18        1  [[0.37732956]]\n",
              "19        1  [[0.37732956]]\n",
              "20        1  [[0.37732956]]\n",
              "21        1  [[0.37732956]]\n",
              "22        1  [[0.37732956]]\n",
              "23        1  [[0.37732956]]\n",
              "24        1  [[0.37732956]]\n",
              "25        1  [[0.37732956]]\n",
              "26        1  [[0.37732956]]\n",
              "27        1  [[0.37732956]]\n",
              "28        1  [[0.37732956]]\n",
              "29        1  [[0.37732956]]\n",
              "..      ...             ...\n",
              "411       0  [[0.37732956]]\n",
              "412       0  [[0.37732956]]\n",
              "413       0  [[0.37732956]]\n",
              "414       0  [[0.37732956]]\n",
              "415       0  [[0.37732956]]\n",
              "416       0  [[0.37732956]]\n",
              "417       0  [[0.37732956]]\n",
              "418       0  [[0.37732956]]\n",
              "419       0  [[0.37732956]]\n",
              "420       0  [[0.37732956]]\n",
              "421       0  [[0.37732956]]\n",
              "422       0  [[0.37732956]]\n",
              "423       0  [[0.37732956]]\n",
              "424       0  [[0.37732956]]\n",
              "425       0  [[0.37732956]]\n",
              "426       0  [[0.37732956]]\n",
              "427       0  [[0.37732956]]\n",
              "428       0  [[0.37732956]]\n",
              "429       0  [[0.37732956]]\n",
              "430       0  [[0.37732956]]\n",
              "431       0  [[0.37732956]]\n",
              "432       0  [[0.37732956]]\n",
              "433       0  [[0.37732956]]\n",
              "434       0  [[0.37732956]]\n",
              "435       0  [[0.37732956]]\n",
              "436       0  [[0.37732956]]\n",
              "437       0  [[0.37732956]]\n",
              "438       0  [[0.37732956]]\n",
              "439       0  [[0.37732956]]\n",
              "440       0  [[0.37732956]]\n",
              "\n",
              "[441 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj7S_2Ddputc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_preds_1=y_preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JSSqtNIvPC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds=[]\n",
        "for i, predicted in enumerate(y_preds_1):\n",
        "  if (predicted[0][0])>=0.4:\n",
        "    value=1\n",
        "    preds.append(value)\n",
        "      \n",
        "    \n",
        "  else :\n",
        "    value=0\n",
        "    preds.append(value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luh0EFqxpt5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_val=Valid_df['Labels']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM4WOJCgqDr1",
        "colab_type": "code",
        "outputId": "a108721e-ad42-403f-a996-b95c54ba7fa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "print(\"\\nConfusion_Marix is :\\n\",confusion_matrix(y_val, preds))\n",
        "print(\"\\nClassification_Report is :\\n\",classification_report(y_val, preds))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Confusion_Marix is :\n",
            " [[250   0]\n",
            " [191   0]]\n",
            "\n",
            "Classification_Report is :\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      1.00      0.72       250\n",
            "           1       0.00      0.00      0.00       191\n",
            "\n",
            "    accuracy                           0.57       441\n",
            "   macro avg       0.28      0.50      0.36       441\n",
            "weighted avg       0.32      0.57      0.41       441\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr-Uk1B-qIJC",
        "colab_type": "code",
        "outputId": "eac0ed94-539f-4527-ca03-a237cf0a8fb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import sklearn.metrics as metrics\n",
        "# calculate the fpr and tpr for all thresholds of the classification\n",
        "\n",
        "fpr, tpr, threshold = metrics.roc_curve(y_val, preds)\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "# method I: plt\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}